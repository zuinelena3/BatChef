% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BatChefParams.R, R/Classes.R
\docType{class}
\name{LimmaParams}
\alias{LimmaParams}
\alias{CombatParams}
\alias{SeuratV3Params}
\alias{SeuratV5Params}
\alias{FastMNNParams}
\alias{HarmonyParams}
\alias{ScanoramaParams}
\alias{ScVIParams}
\alias{ScMerge2Params}
\alias{BBKNNParams}
\alias{LigerParams}
\alias{BatChefParams-class}
\alias{LimmaParams-class}
\alias{CombatParams-class}
\alias{SeuratV3Params-class}
\alias{SeuratV5Params-class}
\alias{FastMNNParams-class}
\alias{HarmonyParams-class}
\alias{ScanoramaParams-class}
\alias{SCVIParams-class}
\alias{ScMerge2Params-class}
\alias{BBKNNParams-class}
\alias{LigerParams-class}
\title{BatChefParams methods}
\usage{
LimmaParams(assay_type = "logcounts", ...)

CombatParams(assay_type = "counts", ...)

SeuratV3Params(
  features,
  pca_name = NULL,
  assay = NULL,
  reference = NULL,
  anchor.features = 2000,
  scale = TRUE,
  normalization.method = "LogNormalize",
  sct.clip.range = NULL,
  reduction = "cca",
  l2.norm = TRUE,
  dims = 1:30,
  k.anchor = 5,
  k.filter = 200,
  k.score = 30,
  max.features = 200,
  nn.method = "annoy",
  n.trees = 50,
  eps = 0,
  verbose = TRUE,
  new.assay.name = "integrated",
  features.to.integrate = NULL,
  k.weight = 100,
  weight.reduction = NULL,
  sd.weight = 1,
  sample.tree = NULL,
  preserve.order = FALSE
)

SeuratV5Params(
  pca_name = NULL,
  method = "CCAIntegration",
  orig.reduction = "pca",
  assay = NULL,
  features = NULL,
  layers = NULL,
  scale.layer = "scale.data",
  new.reduction = "integrated.dr",
  reference = NULL,
  normalization.method = "LogNormalize",
  dims = 1:30,
  k.filter = NA,
  dims.to.integrate = NULL,
  k.weight = 100,
  weight.reduction = NULL,
  sd.weight = 1,
  sample.tree = NULL,
  preserve.order = FALSE,
  verbose = TRUE,
  l2.norm = TRUE,
  k.anchor = 5,
  k.score = 30,
  max.features = 200,
  nn.method = "annoy",
  n.trees = 50,
  eps = 0
)

FastMNNParams(...)

HarmonyParams(...)

ScanoramaParams(assay_type = NULL, return_dimred = FALSE, ...)

ScVIParams(
  layer = NULL,
  labels_key = NULL,
  size_factor_key = NULL,
  categorical_covariate_keys = NULL,
  continuous_covariate_keys = NULL,
  n_hidden = 128,
  n_latent = 10,
  n_layers = 1,
  dropout_rate = 0.1,
  dispersion = "gene",
  gene_likelihood = "zinb",
  latent_distribution = "normal",
  max_epochs = 400,
  accelerator = "auto",
  devices = 1,
  train_size = 0.25,
  validation_size = NULL,
  shuffle_set_split = TRUE,
  load_sparse_tensor = FALSE,
  batch_size = 128,
  early_stopping = FALSE,
  datasplitter_kwargs = NULL,
  plan_kwargs = NULL,
  datamodule = NULL,
  indices = NULL,
  give_mean = TRUE,
  mc_samples = 5000,
  return_dist = FALSE,
  dataloader = NULL,
  transform_batch = NULL,
  gene_list = NULL,
  library_size = 1,
  n_samples = 1,
  n_samples_overall = NULL,
  weights = NULL,
  return_mean = TRUE,
  return_numpy = NULL
)

ScMerge2Params(assay_type = "logcounts", ...)

BBKNNParams(reduction, ...)

LigerParams(
  features,
  useDatasets = NULL,
  verbose = TRUE,
  format.type = NULL,
  remove.missing = NULL,
  method = "iNMF",
  ...
)
}
\arguments{
\item{assay_type}{A string specifying the assay to use for correction.}

\item{...}{Named arguments to pass to individual methods upon dispatch.}

\item{features}{Vector of features to use.}

\item{pca_name}{A string specifying the PCA name.}

\item{assay}{Name of assay for integration.}

\item{reference}{A reference Seurat object.}

\item{anchor.features}{Number of features to be used in anchor finding.}

\item{scale}{A logical to scale the features provided.}

\item{normalization.method}{Name of normalization method used: LogNormalize or SCT.}

\item{sct.clip.range}{Numeric of length two specifying the min and max values the Pearson residual will be clipped to.}

\item{reduction}{A string specifying the name of PCA.}

\item{l2.norm}{Perform L2 normalization on the CCA cell embeddings after dimensional reduction.}

\item{dims}{Number of dimensions of dimensional reduction.}

\item{k.anchor}{Number of neighbors (k) to use when picking anchors.}

\item{k.filter}{Number of anchors to filter.}

\item{k.score}{Number of neighbors (k) to use when scoring anchors.}

\item{max.features}{The maximum number of features to use when specifying the neighborhood search space in the anchor filtering.}

\item{nn.method}{Method for nearest neighbor finding.}

\item{n.trees}{More trees gives higher precision when using annoy approximate nearest neighbor search.}

\item{eps}{Error bound on the neighbor finding algorithm.}

\item{verbose}{Print progress bar/messages}

\item{new.assay.name}{Name for the new assay containing the integrated data.}

\item{features.to.integrate}{Vector of features to integrate.}

\item{k.weight}{Number of neighbors to consider when weighting anchors.}

\item{weight.reduction}{Dimension reduction to use when calculating anchor weights.}

\item{sd.weight}{Controls the bandwidth of the Gaussian kernel for weighting.}

\item{sample.tree}{Specify the order of integration.}

\item{preserve.order}{Do not reorder objects based on size for each pairwise integration.}

\item{method}{iNMF variant algorithm to use for integration.}

\item{orig.reduction}{Name of dimensional reduction for correction.}

\item{layers}{Names of normalized layers in assay.}

\item{scale.layer}{Name(s) of scaled layer(s) in assay.}

\item{new.reduction}{Name of new integrated dimensional reduction.}

\item{dims.to.integrate}{Number of dimensions to return integrated values for.}

\item{return_dimred}{A logical to returning integrated low-dimesional embeddings.}

\item{layer}{layer}

\item{labels_key}{labels_key}

\item{size_factor_key}{size_factor_key}

\item{categorical_covariate_keys}{categorical_covariate_keys}

\item{continuous_covariate_keys}{continuous_covariate_keys}

\item{n_hidden}{n_hidden}

\item{n_latent}{n_latent}

\item{n_layers}{n_layers}

\item{dropout_rate}{dropout_rate}

\item{dispersion}{dispersion}

\item{gene_likelihood}{gene_likelihood}

\item{latent_distribution}{latent_distribution}

\item{max_epochs}{max_epochs}

\item{accelerator}{accelerator}

\item{devices}{devices}

\item{train_size}{train_size}

\item{validation_size}{validation_size}

\item{shuffle_set_split}{shuffle_set_split}

\item{load_sparse_tensor}{load_sparse_tensor}

\item{batch_size}{batch_size}

\item{early_stopping}{early_stopping}

\item{datasplitter_kwargs}{datasplitter_kwargs}

\item{plan_kwargs}{plan_kwargs}

\item{datamodule}{datamodule}

\item{indices}{indices}

\item{give_mean}{give_mean}

\item{mc_samples}{mc_samples}

\item{return_dist}{return_dist}

\item{dataloader}{dataloader}

\item{transform_batch}{transform_batch}

\item{gene_list}{gene_list}

\item{library_size}{library_size}

\item{n_samples}{n_samples}

\item{n_samples_overall}{n_samples_overall}

\item{weights}{weights}

\item{return_mean}{return_mean}

\item{return_numpy}{return_numpy}

\item{useDatasets}{A character vector of the names, a numeric or logical vector of the index of the datasets to be normalized.}

\item{format.type}{Deprecated. The functionality of these is covered through other parts of the whole workflow and is no long needed}

\item{remove.missing}{Deprecated. The functionality of these is covered through other parts of the whole workflow and is no long needed}
}
\description{
Constructors and methods for the params parameter classes.
BatChefParams objects contain method specific parameters to pass to the batchCorrect generic.
}
